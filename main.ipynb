{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import streamlit as st;\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.model_selection import cross_val_score;\n",
    "from sklearn.preprocessing import LabelEncoder;\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# pipeline elements\n",
    "from sklearn.decomposition import PCA # PCA = Principal Component Analysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# pipeline materiaux\n",
    "from sklearn.pipeline import Pipeline # PCA = Principal Component Analysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liens :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger le dataset : \n",
    "   <a href=\"https://drive.google.com/file/d/1OEWrVjE7B2d23-eQrTMcJpRJMxoB6iUH/view?usp=sharing \">labels.csv</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une mesure de performance\n",
    "\n",
    "def accuracy(preds, target):\n",
    "    M = target.shape[0] # Nombre d'exemple\n",
    "    total_correctes = (preds == target).sum()\n",
    "    accuracy = total_correctes / M\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(list):\n",
    "    tmp = 0\n",
    "    proba = 0\n",
    "    for i in range(len(list)):\n",
    "        if list[i] > list[tmp]:\n",
    "            tmp = i\n",
    "            proba = list[i]\n",
    "    return tmp, proba\n",
    "\n",
    "def myClasse(classT, proba):\n",
    "    print(\" Classe : %d avec %f %%. \" % (classT, proba))\n",
    "    st.success(\" Classe : %d avec %f %%. \" % (classT, proba))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kn(number, X_tr, X_te, Y_tr, Y_te):\n",
    "    knn = KNN(n_neighbors=21,\n",
    "             weights='uniform',\n",
    "             leaf_size=3)\n",
    "    knn.fit(X_tr, Y_tr)\n",
    "    train_preds = knn.predict(X_tr)\n",
    "    predictions = knn.predict(X_te)\n",
    "    knnTr = accuracy(train_preds, Y_tr)\n",
    "    knnTe= accuracy(predictions, Y_te)\n",
    "    return knnTr, knnTr\n",
    "    \n",
    "def rf(number, X_tr, X_te, Y_tr, Y_te):\n",
    "    clf = RandomForestClassifier(random_state=0,      # Lecture aléatoire des données\n",
    "                                n_estimators=21,      # Il va diviser X_tr en plusieurs arbres\n",
    "                                max_depth=7)          # Il va spliter/augmenter la profondeur des arbres\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "    train_preds = clf.predict(X_tr)\n",
    "    predictions = clf.predict(X_te)\n",
    "    clfTr = accuracy(train_preds, Y_tr)\n",
    "    clfTe= accuracy(predictions, Y_te)\n",
    "    return clfTr, clfTe\n",
    "    \n",
    "def gb(number, X_tr, X_te, Y_tr, Y_te):\n",
    "    grad = GradientBoostingClassifier(random_state=60,        # Lecture aléatoire des données\n",
    "                                     n_estimators=40,         # Nombre d'étape (modèle), plus il y a de modèle, plus il apprendra\n",
    "                                     max_depth=10)            # Profondeur de l'arborescence\n",
    "    grad.fit(X_tr, Y_tr)\n",
    "    train_preds = grad.predict(X_tr)\n",
    "    predictions = grad.predict(X_te)\n",
    "    gradTr = accuracy(train_preds, Y_tr)\n",
    "    gradTe= accuracy(predictions, Y_te)\n",
    "    return gradTr, gradTe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère notre Dataset et la stocke dans une variable\n",
    "\n",
    "accident = pd.read_csv('data/US_Accidents_June20_mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les colonnes qui nous intéressent pas\n",
    "\n",
    "accident = accident.drop(['ID'],axis=1)\n",
    "accident = accident.drop(['Start_Lat'],axis=1)\n",
    "accident = accident.drop(['Start_Lng'],axis=1)\n",
    "accident = accident.drop(['End_Lat'],axis=1)\n",
    "accident = accident.drop(['End_Lng'],axis=1)\n",
    "accident = accident.drop(['Description'],axis=1)\n",
    "accident = accident.drop(['Number'],axis=1)\n",
    "accident = accident.drop(['Street'],axis=1)\n",
    "accident = accident.drop(['Side'],axis=1)\n",
    "accident = accident.drop(['Zipcode'],axis=1)\n",
    "accident = accident.drop(['Country'],axis=1)\n",
    "accident = accident.drop(['Timezone'],axis=1)\n",
    "accident = accident.drop(['Weather_Timestamp'],axis=1)\n",
    "accident = accident.drop(['Wind_Direction'],axis=1)\n",
    "accident = accident.drop(['Wind_Chill(F)'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève toutes les valeurs NaN\n",
    "\n",
    "accident = accident.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On conserve notre Dataset sans transformation\n",
    "\n",
    "accidentNoTransform = accident.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertie les colonnes dans le type qui nous interessent\n",
    "\n",
    "accident['Source'] = LabelEncoder().fit_transform(accident['Source'])\n",
    "\n",
    "accident.TMC = accident['TMC'].astype('category').cat.codes\n",
    "\n",
    "accident.City = accident['City'].astype('category').cat.codes\n",
    "\n",
    "accident.State = accident['State'].astype('category').cat.codes\n",
    "\n",
    "accident.County = accident['County'].astype('category').cat.codes\n",
    "\n",
    "accident.Airport_Code = accident['Airport_Code'].astype('category').cat.codes\n",
    "\n",
    "accident.Sunrise_Sunset =accident['Sunrise_Sunset'].astype('category').cat.codes \n",
    "\n",
    "accident.Weather_Condition = accident['Weather_Condition'].astype('category').cat.codes\n",
    "\n",
    "accident.Nautical_Twilight = accident['Nautical_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident.Astronomical_Twilight = accident['Astronomical_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident.Civil_Twilight = accident['Civil_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident['Amenity'] = LabelEncoder().fit_transform(accident['Amenity'])\n",
    "accident['Bump'] = LabelEncoder().fit_transform(accident['Bump'])\n",
    "accident['Crossing'] = LabelEncoder().fit_transform(accident['Crossing'])\n",
    "accident['Give_Way'] = LabelEncoder().fit_transform(accident['Give_Way'])\n",
    "accident['Junction'] = LabelEncoder().fit_transform(accident['Junction'])\n",
    "accident['No_Exit'] = LabelEncoder().fit_transform(accident['No_Exit'])\n",
    "accident['Railway'] = LabelEncoder().fit_transform(accident['Railway'])\n",
    "accident['Roundabout'] = LabelEncoder().fit_transform(accident['Roundabout'])\n",
    "accident['Station'] = LabelEncoder().fit_transform(accident['Station'])\n",
    "accident['Stop'] = LabelEncoder().fit_transform(accident['Stop'])\n",
    "accident['Traffic_Calming'] = LabelEncoder().fit_transform(accident['Traffic_Calming'])\n",
    "accident['Traffic_Signal'] = LabelEncoder().fit_transform(accident['Traffic_Signal'])\n",
    "accident['Turning_Loop'] = LabelEncoder().fit_transform(accident['Turning_Loop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va séparer notre target de nos colonnes\n",
    "\n",
    "Y = accident['Severity'].astype('category').cat.codes # La target va être la gravité\n",
    "X = accident.drop('Severity', axis='columns') # En fonction des critère environnant, on va essayer de prédir le niveau de gravité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertie nos column date de type object en type date\n",
    "\n",
    "X['Start_Time'] = pd.to_datetime(X['Start_Time'], \n",
    " format = '%Y-%m-%d %H:%M:%S', \n",
    " errors = 'coerce')\n",
    "X['End_Time'] = pd.to_datetime(X['End_Time'], \n",
    " format = '%Y-%m-%d %H:%M:%S', \n",
    " errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# On créé une colonne pour chaque élément de nos dates de début d'accident\n",
    "\n",
    "X['Start_Time_year'] = X['Start_Time'].dt.year\n",
    "X['Start_Time_month'] = X['Start_Time'].dt.month\n",
    "X['Start_Time_week'] = X['Start_Time'].dt.week\n",
    "X['Start_Time_day'] = X['Start_Time'].dt.day\n",
    "X['Start_Time_hour'] = X['Start_Time'].dt.hour\n",
    "X['Start_Time_minute'] = X['Start_Time'].dt.minute\n",
    "X['Start_Time_dayofweek'] = X['Start_Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# On créé une colonne pour chaque élément de nos dates de fin d'accident\n",
    "\n",
    "X['End_Time_year'] = X['End_Time'].dt.year\n",
    "X['End_Time_month'] = X['End_Time'].dt.month\n",
    "X['End_Time_week'] = X['End_Time'].dt.week\n",
    "X['End_Time_day'] = X['End_Time'].dt.day\n",
    "X['End_Time_hour'] = X['End_Time'].dt.hour\n",
    "X['End_Time_minute'] = X['End_Time'].dt.minute\n",
    "X['End_Time_dayofweek'] = X['End_Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant que l'on a créé nos colonnes, on supprime nos de base vue que l'on en a plus besoin\n",
    "\n",
    "X = X.drop(['Start_Time'],axis=1)\n",
    "X = X.drop(['End_Time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève toutes les valeurs NaN\n",
    "\n",
    "X = X.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25482, 46), (25482,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = TTS(X, Y,              # features, target\n",
    "                            stratify = Y,       # Va prendre une proportion aux hasard de valeurs différentes histoire de ne pas avoir des cas où l'on a que des même valeur\n",
    "                            random_state=777,   # Sert à fixer le harsard pour ne pas avoir des résultat différents à chaque tests.\n",
    "                            train_size=0.8)     # 50% de X_train et Y_train et 50% de Y_test et Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(n_neighbors=21,\n",
    "         weights='uniform',\n",
    "         leaf_size=3)\n",
    "knn.fit(X_tr, Y_tr)\n",
    "train_preds = knn.predict(X_tr)\n",
    "predictions = knn.predict(X_te)\n",
    "knnTr = accuracy(train_preds, Y_tr)\n",
    "knnTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,      # Lecture aléatoire des données\n",
    "                            n_estimators=21,      # Il va diviser X_tr en plusieurs arbres\n",
    "                            max_depth=7)          # Il va spliter/augmenter la profondeur des arbres\n",
    "clf.fit(X_tr, Y_tr)\n",
    "train_preds = clf.predict(X_tr)\n",
    "predictions = clf.predict(X_te)\n",
    "clfTr = accuracy(train_preds, Y_tr)\n",
    "clfTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier(random_state=60,        # Lecture aléatoire des données\n",
    "                                 n_estimators=40,         # Nombre d'étape (modèle), plus il y a de modèle, plus il apprendra\n",
    "                                 max_depth=10)            # Profondeur de l'arborescence\n",
    "grad.fit(X_tr, Y_tr)\n",
    "train_preds = grad.predict(X_tr)\n",
    "predictions = grad.predict(X_te)\n",
    "gradTr = accuracy(train_preds, Y_tr)\n",
    "gradTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/prediction/{algo}/\")\n",
    "async def prediction(algo: str):\n",
    "    if algo == 'KNN':\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, knnTr, knnTe)\n",
    "    elif algo == 'RF':\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, clfTr, clfTe)\n",
    "    elif algo == 'GB':\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, gradTr, gradTe)\n",
    "    else:\n",
    "        result = \" Vous devez choisir entre : KNN, RF (RandomForest) ou GB (GradientBoosting)\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/prediction/{algo}/{number}/\")\n",
    "async def prediction(algo: str, number: float):\n",
    "    if number > 1 or number < 0:\n",
    "        result = \" Vous devez choisir entre : KNN, RF (RandomForest) ou GB (GradientBoosting) en plus du pourcentage de splitting entre le train et le test (train_size) entre 1 et 0.\"\n",
    "    else:\n",
    "        X_tr, X_te, Y_tr, Y_te = TTS(X, Y,              # features, target\n",
    "                            stratify = Y,       # Va prendre une proportion aux hasard de valeurs différentes histoire de ne pas avoir des cas où l'on a que des même valeur\n",
    "                            random_state=777,   # Sert à fixer le harsard pour ne pas avoir des résultat différents à chaque tests.\n",
    "                            train_size=number)     # 50% de X_train et Y_train et 50% de Y_test et Y_test\n",
    "    \n",
    "    if algo == 'KNN':\n",
    "        scoreTr, scoreTe = kn(number, X_tr, X_te, Y_tr, Y_te)\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, scoreTr, scoreTe)\n",
    "    elif algo == 'RF':\n",
    "        scoreTr, scoreTe = rf(number, X_tr, X_te, Y_tr, Y_te)\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, scoreTr, scoreTe)\n",
    "    elif algo == 'GB':\n",
    "        scoreTr, scoreTe = gb(number, X_tr, X_te, Y_tr, Y_te)\n",
    "        result = \" Pour l\\'algorithme %s, on à un score de %f d'accuracy en train et %f d'accuracy en test. \" % (algo, scoreTr, scoreTe)\n",
    "    else:\n",
    "        result = \" Vous devez choisir entre : KNN, RF (RandomForest) ou GB (GradientBoosting) en plus du pourcentage de splitting entre le train et le test (train_size) entre 1 et 0.\"\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
